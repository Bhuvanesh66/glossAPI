# -*- coding: utf-8 -*-
"""GreekBERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rZfWqVzpcbShaznAeybMiGgUAaAOVhOK
"""

pip install transformers datasets torch

pip install transformers[torch] accelerate -U

"""### Reading Origional Dataset"""

import pandas as pd
data_for_BERT=pd.read_csv("/content/drive/MyDrive/ODECO/Secondments/GFOSS Secondment Justification/GreekBERT/data_for_BERT.csv")
data_for_BERT.drop(columns="Unnamed: 0", inplace=True)

data_for_BERT=data_for_BERT.iloc[:-467]

data_for_BERT.text[1600]

"""### Experiments starts from here"""

data_for_BERT

data_for_BERT.label.value_counts()

print(data_for_BERT.isnull().sum())

data_for_BERT = data_for_BERT.dropna()

import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
import torch
import torch.nn.functional as F



# Split the dataset into train and validation sets
train_data, val_data = train_test_split(data_for_BERT, test_size=0.3, random_state=42)

# Load the GreekBERT tokenizer
tokenizer = AutoTokenizer.from_pretrained("nlpaueb/bert-base-greek-uncased-v1")

# Tokenize the data
def tokenize_data(data):
    return tokenizer(data["text"].tolist(), padding='max_length', truncation=True, max_length=512) #with 512 session was crashing

train_encodings = tokenize_data(train_data)
val_encodings = tokenize_data(val_data)

train_data

train_labels = train_data["label"].tolist()
val_labels = val_data["label"].tolist()

# Convert the encoded inputs and labels to PyTorch tensors
class Dataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = Dataset(train_encodings, train_labels)
val_dataset = Dataset(val_encodings, val_labels)

num_labels= max(data_for_BERT["label"]) + 1

# Load the pre-trained GreekBERT model for sequence classification
model = AutoModelForSequenceClassification.from_pretrained("nlpaueb/bert-base-greek-uncased-v1", num_labels=4)

pip install accelerate -U

# Define the training arguments
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=10,
    per_device_train_batch_size=8, #with 16 also session was crashing
    per_device_eval_batch_size=8,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir="./logs",
     gradient_accumulation_steps=8,  # Simulate a larger batch size
)

# Define the function to compute the accuracy
def compute_accuracy(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    return {"accuracy": (preds == labels).mean()}

# Define the trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_accuracy,
)

# Training loop
trainer.train()

# Evaluate the model
trainer.evaluate()

# Access the trained model from the Trainer object
trained_model = trainer.model

# Save the trained model
trained_model.save_pretrained("/content/drive/MyDrive/ODECO/Secondments/GFOSS Secondment Justification/GreekBERT/updated/saved_model")

from transformers import BertForSequenceClassification

# Load the saved model for prediction
loaded_model = BertForSequenceClassification.from_pretrained("/content/drive/MyDrive/ODECO/Secondments/GFOSS Secondment Justification/GreekBERT//updated//saved_model")

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained("nlpaueb/bert-base-greek-uncased-v1")

# Define a function to tokenize a single string
def tokenize_single_string(text):
    # Tokenize the input text
    tokenized_input = tokenizer(text, padding='max_length', truncation=True, max_length=512, return_tensors="pt")
    return tokenized_input

# Example usage with a single string
input_text = "Για να προκόψη η επιστήμη, για να μάθη χωριανέ."

inputs = tokenize_single_string(input_text)

import torch
from transformers import BertTokenizer, BertForSequenceClassification

# Load the tokenizer for Greek text
tokenizer = BertTokenizer.from_pretrained("nlpaueb/bert-base-greek-uncased-v1")

# Define the input text in Greek
input_text = "Μια και δυο κίνησε και πήγε στο σπίτι της Μαχο"


# Tokenize the input text
inputs = tokenize_single_string(input_text)

# Perform inference
with torch.no_grad():
    outputs = loaded_model(**inputs)

# Interpret the prediction
predicted_class = torch.argmax(outputs.logits, dim=1).item()
print("Predicted class:", predicted_class)

import pandas as pd

data_for_predictions=pd.read_excel("/content/drive/MyDrive/ODECO/Secondments/GFOSS Secondment Justification/GreekBERT/New Predictions/PostBERT.xlsx")

data_for_predictions.drop(columns="Class", inplace=True)

predictions = []

data_for_predictions

def predict(text):
    # Tokenize the input text
    inputs = tokenize_single_string(text)
    # print(text)
    # Perform inference
    with torch.no_grad():
        outputs = loaded_model(**inputs)

    # Interpret the prediction
    predicted_class = torch.argmax(outputs.logits, dim=1).item()
    return predicted_class
    # print("Predicted class:", predicted_class)

# Apply the function to the DataFrame column
data_for_predictions['predicted_class'] = data_for_predictions['Text'].apply(predict)

data_for_predictions.predicted_class.value_counts()

# Convert DataFrame to dictionary
data_for_predictions_dict = data_for_predictions.to_dict(orient='list')

# Print the resulting dictionary
print("Dictionary:")
print(data_for_predictions_dict)

import pickle
# Define the filename for the pickle file
pickle_file = r'/content/drive/MyDrive/ODECO/Secondments/GFOSS Secondment Justification/GreekBERT/New Predictions/data_for_predictions_dict.pickle'

# Store the dictionary to pickle file
with open(pickle_file, 'wb') as f:
    pickle.dump(data_for_predictions_dict, f)

print(f"Dictionary stored to '{pickle_file}'")

# Load the dictionary from pickle file
with open(pickle_file, 'rb') as f:
    loaded_dict = pickle.load(f)

# Print the loaded dictionary
print("Loaded Dictionary:")
print(loaded_dict)

