
@book{pustejovsky_natural_2013,
	address = {Beijing Köln},
	edition = {1. ed},
	title = {Natural language annotation for machine learning: a guide to corpus-building for applications},
	isbn = {978-1-4493-0666-3},
	shorttitle = {Natural language annotation for machine learning},
	language = {en},
	publisher = {O'Reilly},
	author = {Pustejovsky, James and Stubbs, Amber},
	year = {2013},
	file = {Pustejovsky and Stubbs - 2013 - Natural language annotation for machine learning .pdf:/home/ninagial/Zotero/storage/RIC27YAH/Pustejovsky and Stubbs - 2013 - Natural language annotation for machine learning .pdf:application/pdf},
}

@misc{dettmers_qlora_2023,
	title = {{QLoRA}: {Efficient} {Finetuning} of {Quantized} {LLMs}},
	shorttitle = {{QLoRA}},
	url = {http://arxiv.org/abs/2305.14314},
	abstract = {We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters{\textasciitilde}(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3\% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.},
	language = {en},
	urldate = {2023-11-02},
	publisher = {arXiv},
	author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
	month = may,
	year = {2023},
	note = {arXiv:2305.14314 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Dettmers et al. - 2023 - QLoRA Efficient Finetuning of Quantized LLMs.pdf:/home/ninagial/Zotero/storage/8D2N2NJ2/Dettmers et al. - 2023 - QLoRA Efficient Finetuning of Quantized LLMs.pdf:application/pdf},
}

@misc{piktus_roots_2023,
	title = {The {ROOTS} {Search} {Tool}: {Data} {Transparency} for {LLMs}},
	shorttitle = {The {ROOTS} {Search} {Tool}},
	url = {http://arxiv.org/abs/2302.14035},
	abstract = {ROOTS is a 1.6TB multilingual text corpus developed for the training of BLOOM, currently the largest language model explicitly accompanied by commensurate data governance efforts. In continuation of these efforts, we present the ROOTS Search Tool: a search engine over the entire ROOTS corpus offering both fuzzy and exact search capabilities. ROOTS is the largest corpus to date that can be investigated this way. The ROOTS Search Tool is open-sourced and available on Hugging Face Spaces. We describe our implementation and the possible use cases of our tool.},
	language = {en},
	urldate = {2023-11-02},
	publisher = {arXiv},
	author = {Piktus, Aleksandra and Akiki, Christopher and Villegas, Paulo and Laurençon, Hugo and Dupont, Gérard and Luccioni, Alexandra Sasha and Jernite, Yacine and Rogers, Anna},
	month = feb,
	year = {2023},
	note = {arXiv:2302.14035 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Piktus et al. - 2023 - The ROOTS Search Tool Data Transparency for LLMs.pdf:/home/ninagial/Zotero/storage/E6LSYMHR/Piktus et al. - 2023 - The ROOTS Search Tool Data Transparency for LLMs.pdf:application/pdf},
}

@misc{zhou_lima_2023,
	title = {{LIMA}: {Less} {Is} {More} for {Alignment}},
	shorttitle = {{LIMA}},
	url = {http://arxiv.org/abs/2305.11206},
	abstract = {Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43\% of cases; this statistic is as high as 58\% when compared to Bard and 65\% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.},
	urldate = {2023-11-02},
	publisher = {arXiv},
	author = {Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srini and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and Zhang, Susan and Ghosh, Gargi and Lewis, Mike and Zettlemoyer, Luke and Levy, Omer},
	month = may,
	year = {2023},
	note = {arXiv:2305.11206 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/ninagial/Zotero/storage/8P7WGYLG/Zhou et al. - 2023 - LIMA Less Is More for Alignment.pdf:application/pdf;arXiv.org Snapshot:/home/ninagial/Zotero/storage/5MFHSZAV/2305.html:text/html},
}

@misc{noauthor_using_2023,
	title = {Using {LLaMA} 2.0, {FAISS} and {LangChain} for {Question}-{Answering} on {Your} {O}…},
	url = {https://archive.ph/dmGAR},
	urldate = {2023-11-02},
	journal = {archive.ph},
	month = aug,
	year = {2023},
	file = {Snapshot:/home/ninagial/Zotero/storage/ZG442EM6/dmGAR.html:text/html},
}

@misc{noauthor_langchain_2023,
	title = {{LangChain} {QuickStart} with {Llama} 2 {\textbar} by {Venelin} {Valkov} {\textbar} {Sep}, 2023 {\textbar} {M}…},
	url = {https://archive.ph/i7E3R},
	urldate = {2023-11-02},
	journal = {archive.ph},
	month = sep,
	year = {2023},
}

@misc{noauthor_running_nodate,
	title = {Running {Llama} 2 on {CPU} {Inference} {Locally} for {Document} {Q}\&{A} {\textbar} by {Kenneth} {Leung} {\textbar} {Towards} {Data} {Science}},
	url = {https://archive.ph/7iYlC},
	urldate = {2023-11-02},
	file = {Running Llama 2 on CPU Inference Locally for Document Q&A | by Kenneth Leung | Towards Data Science:/home/ninagial/Zotero/storage/4J2BTD2I/7iYlC.html:text/html},
}

@misc{noauthor_maximum_nodate,
	title = {Maximum {Likelihood} {Decoding} with {RNNs} - the good, the bad, and the ugly - {The} {Stanford} {Natural} {Language} {Processing} {Group}},
	url = {https://nlp.stanford.edu/blog/maximum-likelihood-decoding-with-rnns-the-good-the-bad-and-the-ugly/},
	urldate = {2023-11-02},
	file = {Maximum Likelihood Decoding with RNNs - the good, the bad, and the ugly - The Stanford Natural Language Processing Group:/home/ninagial/Zotero/storage/LKV8Z8UP/maximum-likelihood-decoding-with-rnns-the-good-the-bad-and-the-ugly.html:text/html},
}

@misc{noauthor_machine_nodate,
	title = {machine learning - {The} effect of temperature in temperature sampling - {Cross} {Validated}},
	url = {https://stats.stackexchange.com/questions/255223/the-effect-of-temperature-in-temperature-sampling},
	urldate = {2023-11-02},
	file = {machine learning - The effect of temperature in temperature sampling - Cross Validated:/home/ninagial/Zotero/storage/BBDGL8AD/the-effect-of-temperature-in-temperature-sampling.html:text/html},
}

@misc{noauthor_language_2023,
	title = {Language {Model} {Evaluation} {Harness}},
	copyright = {MIT},
	url = {https://github.com/EleutherAI/lm-evaluation-harness},
	abstract = {A framework for few-shot evaluation of autoregressive language models.},
	urldate = {2023-11-02},
	publisher = {EleutherAI},
	month = nov,
	year = {2023},
	note = {original-date: 2020-08-28T00:09:15Z},
}
